{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnEf299z3qfE"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J8vbzmkeI4_i"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3yxSrGp3u-E"
   },
   "source": [
    "# Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VwtoWYsW3xOt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, decision_size, decision_overlap, segments_size=90, segments_overlap=45, sampling=2):\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.sampling = sampling\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "    def transfer(self, dataset, features, method):\n",
    "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
    "        segments, labels = self.__segment_signal(dataset, features)\n",
    "        print(\"making \" + str(len(segments)) + \" segments\")\n",
    "        if method == \"table\":\n",
    "            segments_dataset = self.__transfer_table(segments, features)\n",
    "        elif method == \"1d\":\n",
    "            segments_dataset = self.__transfer_1d(segments, features)\n",
    "        elif method == \"2d\":\n",
    "            segments_dataset = self.__transfer_2d(segments, features)\n",
    "        elif method == \"3d_1ch\":\n",
    "            segments_dataset = self.__transfer_2d_1ch(segments, features)\n",
    "        elif method == \"3d\":\n",
    "            segments_dataset = self.__transfer_3d(segments, features)\n",
    "        elif method == \"4d\":\n",
    "            segments_dataset = self.__transfer_4d(segments, features)\n",
    "        elif method == \"rnn_2d\":\n",
    "            segments_dataset, labels = self.__transfer_rnn_2d(segments, labels, features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            segments_dataset, labels = self.__transfer_rnn_3d_1ch(segments, labels, features)\n",
    "        return segments_dataset, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def data_shape(method, n_features, segments_size, segments_overlap=None, decision_size=None):\n",
    "        if method == \"table\":\n",
    "            return (None, n_features * segments_size)\n",
    "        elif method == \"1d\":\n",
    "            return (None, 1, n_features * segments_size, 1)\n",
    "        elif method == \"2d\":\n",
    "            return (None, n_features, segments_size)\n",
    "        elif method == \"3d_1ch\":\n",
    "            return (None, n_features, segments_size, 1)\n",
    "        elif method == \"3d\":\n",
    "            return (None, 1, segments_size, n_features)\n",
    "        elif method == \"4d\":\n",
    "            return (n_features, None, 1, segments_size, 1)\n",
    "        elif method == \"rnn_2d\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, segments_size * n_features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, 1, segments_size * n_features)\n",
    "        return ()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_segments_a_decision_window(segment_size, segment_overlap_size, decision_size):\n",
    "        return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)\n",
    "\n",
    "    def __transfer_rnn_2d(self, segments, labels, features):\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s - r) + j]\n",
    "                    A = A.reshape(A.shape[0] * A.shape[1])\n",
    "                    row.append(A)\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_rnn_3d_1ch(self, segments, labels, features):\n",
    "        # (samples, time, channels=1, rows)\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s - r) + j]\n",
    "                    A = A.reshape(A.shape[0] * A.shape[1])\n",
    "                    row.append([A])\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_table(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_1d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_2d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_3d_1ch(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_3d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for i in range(len(segment[0])):\n",
    "                cell = []\n",
    "                for feature_i in range(len(features)):\n",
    "                    cell.append(segment[feature_i][i])\n",
    "                row.append(cell)\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_4d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for feature_i in range(len(features)):\n",
    "            row = []\n",
    "            for segment in segments:\n",
    "                cell = []\n",
    "                for element in segment[feature_i]:\n",
    "                    cell.append([element])\n",
    "                row.append([cell])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __windows(self, data):\n",
    "        start = 0\n",
    "        while start < data.count():\n",
    "            yield int(start), int(start + self.segments_size)\n",
    "            start += (self.segments_size - self.segments_overlap)\n",
    "\n",
    "    def __segment_signal(self, dataset, features):\n",
    "        segments = []\n",
    "        labels = []\n",
    "        for class_i in np.unique(dataset[\"id\"]):\n",
    "            subset = dataset[dataset[\"id\"] == class_i]\n",
    "            for (start, end) in self.__windows(subset[\"id\"]):\n",
    "                feature_slices = []\n",
    "                for feature in features:\n",
    "                    feature_slices.append(subset[feature][start:end].tolist())\n",
    "                if len(feature_slices[0]) == self.segments_size:\n",
    "                    segments.append(feature_slices)\n",
    "                    labels.append(class_i)\n",
    "        return np.array(segments), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsqjDnvY30UY"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RisgNiya33cE"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        \"\"\"\n",
    "        :param db_path:\n",
    "        :param sample_rate:\n",
    "        :param features:\n",
    "        :param window_time: in seconds\n",
    "        :param window_overlap_percentage: example: 0.75 for 75%\n",
    "        :param add_noise: True or False\n",
    "        :param noise_rate:\n",
    "        :param train_blocks:\n",
    "        :param valid_blocks:\n",
    "        :param test_blocks:\n",
    "        :param data_length_time: the amount of data from each class in seconds. -1 means whole existing data.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.features = features\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "\n",
    "        # Initialization\n",
    "        self.train_dataset = pd.DataFrame()\n",
    "        self.valid_dataset = pd.DataFrame()\n",
    "        self.test_dataset = pd.DataFrame()\n",
    "        self.n_train_dataset = pd.DataFrame()\n",
    "        self.n_valid_dataset = pd.DataFrame()\n",
    "        self.n_test_dataset = pd.DataFrame()\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "    def load_data(self, n_classes, method, read_cache=True, random_selection=True):\n",
    "        segments_path = \"./\" + self.db_path.split(\"/\")[-2] + \"/\" + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method: \" + str(method) + os.sep + \\\n",
    "                        \"noise: \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes:\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl: \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo: \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl: \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do: \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train: \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid: \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test: \" + str(self.test_blocks) + os.sep\n",
    "        print(segments_path)\n",
    "        if read_cache \\\n",
    "                and os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            self.__preprocess(n_classes, method, random_selection)\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "        def to_dic(data):\n",
    "            dic = {}\n",
    "            for i, x in enumerate(data):\n",
    "                dic[str(i)] = x\n",
    "            return dic\n",
    "\n",
    "        if len(self.X_train.shape) == 5:\n",
    "            self.X_train = to_dic(self.X_train)\n",
    "            self.X_valid = to_dic(self.X_valid)\n",
    "            self.X_test = to_dic(self.X_test)\n",
    "\n",
    "    def __preprocess(self, n_classes, method, random_selection=True):\n",
    "        if random_selection:\n",
    "            csv_paths = np.random.choice(glob.glob(self.db_path + \"*.csv\"), n_classes, replace=False)\n",
    "        else:\n",
    "            csv_paths = glob.glob(self.db_path + \"*.csv\")[:n_classes]\n",
    "\n",
    "        self.class_names = {}\n",
    "        for i, csv_path in enumerate(csv_paths):\n",
    "            label = os.path.basename(csv_path).split('.')[0]\n",
    "            self.class_names[label] = i\n",
    "            train, valid, test = self.__read_data(csv_path, self.features, label)\n",
    "            train['id'] = i\n",
    "            valid['id'] = i\n",
    "            test['id'] = i\n",
    "            self.train_dataset = pd.concat([self.train_dataset, train])\n",
    "            self.valid_dataset = pd.concat([self.valid_dataset, valid])\n",
    "            self.test_dataset = pd.concat([self.test_dataset, test])\n",
    "\n",
    "        self.__standardization()\n",
    "        self.__segmentation(method=method)\n",
    "\n",
    "    def __read_data(self, path, features, label):\n",
    "        data = pd.read_csv(path, low_memory=False)\n",
    "        data = data[features]\n",
    "        data = data.fillna(data.mean())\n",
    "        length = self.data_length_size if self.data_length_size != -1 else data.shape[0]\n",
    "        print('class: %5s, data size: %s, selected data size: %s' % (\n",
    "            label, str(timedelta(seconds=int(data.shape[0] / self.sample_rate))),\n",
    "            str(timedelta(seconds=int(length / self.sample_rate)))))\n",
    "        return self.__split_to_train_valid_test(data)\n",
    "\n",
    "    def __split_to_train_valid_test(self, data):\n",
    "        n_blocks = max(self.train_blocks + self.valid_blocks + self.test_blocks) + 1\n",
    "        block_length = int(len(data[:self.data_length_size]) / n_blocks)\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        for i in range(len(self.train_blocks)):\n",
    "            start = self.train_blocks[i] * block_length\n",
    "            end = self.train_blocks[i] * block_length + block_length - 1\n",
    "            if train_data.empty:\n",
    "                train_data = data[start:end]\n",
    "            else:\n",
    "                train_data = pd.concat([data[start:end], train_data])\n",
    "\n",
    "        valid_data = pd.DataFrame()\n",
    "        for i in range(len(self.valid_blocks)):\n",
    "            start = self.valid_blocks[i] * block_length\n",
    "            end = self.valid_blocks[i] * block_length + block_length - 1\n",
    "            if valid_data.empty:\n",
    "                valid_data = data[start:end]\n",
    "            else:\n",
    "                valid_data = pd.concat([data[start:end], valid_data])\n",
    "\n",
    "        test_data = pd.DataFrame()\n",
    "        for i in range(len(self.test_blocks)):\n",
    "            start = self.test_blocks[i] * block_length\n",
    "            end = self.test_blocks[i] * block_length + block_length - 1\n",
    "            if test_data.empty:\n",
    "                test_data = data[start:end]\n",
    "            else:\n",
    "                test_data = pd.concat([data[start:end], test_data])\n",
    "\n",
    "        if self.add_noise:\n",
    "            test_data = self.__add_noise_to_data(test_data)\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def __add_noise_to_data(self, x):\n",
    "        x_power = x ** 2\n",
    "        sig_avg_watts = np.mean(x_power)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - self.target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), size=x.shape)\n",
    "        return x + noise_volts\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.train_dataset.iloc[:, :-1])\n",
    "        n_train_dataset = scaler.transform(self.train_dataset.iloc[:, :-1])\n",
    "        n_valid_dataset = scaler.transform(self.valid_dataset.iloc[:, :-1])\n",
    "        n_test_dataset = scaler.transform(self.test_dataset.iloc[:, :-1])\n",
    "\n",
    "        self.n_train_dataset = pd.DataFrame(n_train_dataset, columns=self.features)\n",
    "        self.n_valid_dataset = pd.DataFrame(n_valid_dataset, columns=self.features)\n",
    "        self.n_test_dataset = pd.DataFrame(n_test_dataset, columns=self.features)\n",
    "        self.n_train_dataset['id'] = self.train_dataset.iloc[:, -1].tolist()\n",
    "        self.n_valid_dataset['id'] = self.valid_dataset.iloc[:, -1].tolist()\n",
    "        self.n_test_dataset['id'] = self.test_dataset.iloc[:, -1].tolist()\n",
    "\n",
    "    def __segmentation(self, method):\n",
    "        transformer = Transformer(segments_size=self.window_size,\n",
    "                                  segments_overlap=self.window_overlap_size,\n",
    "                                  decision_size=self.decision_size,\n",
    "                                  decision_overlap=self.decision_overlap_percentage)\n",
    "        self.X_train, self.y_train = transformer.transfer(self.n_train_dataset, self.features, method=method)\n",
    "        self.X_valid, self.y_valid = transformer.transfer(self.n_valid_dataset, self.features, method=method)\n",
    "        self.X_test, self.y_test = transformer.transfer(self.n_test_dataset, self.features, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAGp_3s364X"
   },
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cNfOUKPl4Crf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pesl(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = (c - 1) / (c ** 2)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    return tf.math.reduce_mean(tf.math.reduce_mean(tf.math.square(tf.math.subtract(y, q)), axis=1) + payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esl(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.mean_squared_error(y,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pll(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = math.log(1/c)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    log_loss = tf.keras.losses.categorical_crossentropy(y,q)\n",
    "    p_log_loss = tf.cast(log_loss, tf.float32) - payoff\n",
    "    return tf.math.reduce_mean(p_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(y,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KP0pEQM4KNT"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2prG7BJT4Kt5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    # Define the true positives, false positives and false negatives\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    # Calculate the precision and recall\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jM3l01B4R-Y"
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "byq18z6w4K5r"
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        return self.loss_function\n",
    "\n",
    "    def get_loss_function_name(self):\n",
    "        return self.loss_function if type(self.loss_function) == str else self.loss_function.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "95NTs4jw4aN8"
   },
   "outputs": [],
   "source": [
    "# Convolutinal Neural Network\n",
    "class CNN_L(Classifier):\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap,\n",
    "                 decision_size, decision_overlap,\n",
    "                 loss_function, loss_metric, lr=0.0001, beta_1=0.5):\n",
    "        super().__init__(loss_metric)\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        # 'categorical_crossentropy'\n",
    "        optimizer = tf.keras.optimizers.Nadam(lr, beta_1)\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(input_)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(1024, kernel_initializer=self.initializer, bias_initializer=self.initializer)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, callback, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        history = self.model.fit(X_train, y_train_onehot,\n",
    "                                 validation_data=(X_valid, y_valid_onehot),\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=1,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=callback)\n",
    "\n",
    "        return history, self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJsGcm6jBgAL"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "gKIV9uFSCnKA"
   },
   "outputs": [],
   "source": [
    "class ModelAnalyser:\n",
    "\n",
    "    def __init__(self, segment_size, segment_overlap, decision_size, decision_overlap, X, y):\n",
    "        \"\"\"\n",
    "        :param segment_size: for calculating number of segments in a decision window.\n",
    "        :param segment_overlap: for calculating number of segments in a decision window.\n",
    "        :param decision_size:\n",
    "        :param decision_overlap:\n",
    "        :param X:\n",
    "        :param y:\n",
    "        \"\"\"\n",
    "        self.segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.X = X\n",
    "        self.y_real = y\n",
    "\n",
    "    def measurement(self, model, monitor):\n",
    "        \"\"\"\n",
    "        :param y_real: expected labels\n",
    "        :param model: the core\n",
    "        :param monitor: '#METHOD_#MEASURE', #METHOD={'mv','ms'}, #MEASURE={'loss','accuracy','precision','recall','f1'}\n",
    "        \"\"\"\n",
    "        y_prediction = model.predict(self.X)\n",
    "        \n",
    "        monitor_method = monitor.split('_')[0]\n",
    "        monitor_measure = monitor.split('_')[1]\n",
    "\n",
    "        if monitor_method == 'ms':\n",
    "            y_pred_labels, y_dw_real = AveragingProbabilities(y_truth=self.y_real,\n",
    "                                                              y_prediction=y_prediction,\n",
    "                                                              s=self.segments_a_decision_window,\n",
    "                                                              r=self.decision_overlap)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'confusionmatrix':\n",
    "                y_dw_real_category = [ np.argmax(t) for t in y_dw_real ]\n",
    "                y_pred_one_hot_category = [ np.argmax(t) for t in y_pred_one_hot ]\n",
    "                return confusion_matrix(y_dw_real_category, y_pred_one_hot_category)\n",
    "\n",
    "        if monitor_method == 'mv':\n",
    "            y_pred_labels, y_dw_real = MajorityVote(y_truth=self.y_real,\n",
    "                                                    y_prediction=y_prediction,\n",
    "                                                    s=self.segments_a_decision_window,\n",
    "                                                    r=self.decision_overlap,\n",
    "                                                    weights=value_coef)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'confusionmatrix':\n",
    "                y_dw_real_category = [ np.argmax(t) for t in y_dw_real ]\n",
    "                y_pred_one_hot_category = [ np.argmax(t) for t in y_pred_one_hot ]\n",
    "                return confusion_matrix(y_dw_real, y_pred_one_hot)\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MvEjO6S5Bhdq"
   },
   "outputs": [],
   "source": [
    "class RestoringBest(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    :param monitor: '#METHOD_#MEASURE', #METHOD={'mv','ms'}, #MEASURE={'loss','accuracy','precision','recall','f1'}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric: ModelAnalyser, mode, monitor):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        if mode == 'min':\n",
    "            self.best = np.Inf\n",
    "        else:\n",
    "            self.best = -np.Inf\n",
    "        self.best_weights = None\n",
    "        self.best_epoch = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = self.metric.measurement(model=self.model, monitor=self.monitor)\n",
    "\n",
    "        accuracy = self.metric.measurement(model=self.model, monitor=\"ms_accuracy\")\n",
    "        f1 = self.metric.measurement(model=self.model, monitor=\"ms_f1\")\n",
    "\n",
    "        print('epoch %d: \\t %s: %f \\t %s: %f \\t %s: %f' % (epoch+1, self.monitor, current, \"accuracy\", accuracy, \"f1\", f1))\n",
    "\n",
    "        monitor_measure = self.monitor.split('_')[1]\n",
    "\n",
    "        if (self.mode == 'max' and current > self.best) or (self.mode == 'min' and current < self.best):\n",
    "            self.best = current\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_epoch > -1:\n",
    "            print(\"Restoring model weights from the end of the %d epoch.\" % (self.best_epoch + 1))\n",
    "            self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIPRTjzc4eLw"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Ltka_UrJ4hcy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def analysis_model(loss_fn, loss_name, y_pred, y_real_raw, segment_size, segment_overlap, decision_size, decision_overlap):\n",
    "    result = {'Core': {}, 'MV': {}, 'MS': {}}\n",
    "    result['Core']['pesl'] = pesl(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    result['Core']['mse'] = tf.keras.metrics.mean_squared_error(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy().mean()\n",
    "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "    result['Core']['accuracy'] = accuracy_score(y_real_raw, y_pred_arg)\n",
    "    result['Core']['precision'] = precision_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['recall'] = recall_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['f1'] = f1_score(y_real_raw, y_pred_arg, average='macro')\n",
    "\n",
    "    segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "\n",
    "    # Maximum Score\n",
    "    y_pred_labels, y_real = AveragingProbabilities(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                                   s=segments_a_decision_window,\n",
    "                                                   r=decision_overlap)\n",
    "\n",
    "    result['MS']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MS']['mse'] = tf.keras.metrics.mean_squared_error(y_real, y_pred_labels).numpy().mean()\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MS']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MS']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    # Majority Voting\n",
    "    y_pred_labels, y_real = MajorityVote(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                         s=segments_a_decision_window,\n",
    "                                         r=decision_overlap)\n",
    "\n",
    "    result['MV']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MV']['mse'] = tf.keras.metrics.mean_squared_error(y_real, y_pred_labels).numpy().mean()\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MV']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MV']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "DZDd-uxHdzY-"
   },
   "outputs": [],
   "source": [
    "def get_segments_a_decision_window(segment_size, segment_overlap, decision_size):\n",
    "    segment_overlap_size = segment_size * segment_overlap\n",
    "    return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "JJWrMn0Nd5gm"
   },
   "outputs": [],
   "source": [
    "def AveragingProbabilities(y_truth, y_prediction, s, r):\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_prediction[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            for j in range(s):\n",
    "                row += subset[(i * s) + j]\n",
    "            df.append(row / s)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "0OxO2RAVd6-D"
   },
   "outputs": [],
   "source": [
    "def MajorityVote(y_truth, y_prediction, s, r):\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    # Make prior prediction to one-hot\n",
    "    y_categorical_pred = np.zeros_like(y_prediction)\n",
    "    y_categorical_pred[np.arange(len(y_prediction)), y_prediction.argmax(1)] = 1\n",
    "\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_categorical_pred[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            for j in range(s):\n",
    "                row += subset[(i * s) + j]\n",
    "            df.append(row / s)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDN5yNvo4jZ0"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "nVjqWBgQ4mBR"
   },
   "outputs": [],
   "source": [
    "def train_model(dataset: Dataset, classifier: Classifier, epochs, batch_size,\n",
    "                log_dir, monitor_metric, monitor_mode, restore_best=True):\n",
    "\n",
    "    callbacks = []\n",
    "    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    callbacks.append(tbCallBack)\n",
    "\n",
    "    if restore_best:\n",
    "        metric = ModelAnalyser(segment_size = classifier.segments_size,\n",
    "                               segment_overlap = classifier.segments_overlap,\n",
    "                               decision_size = classifier.decision_size,\n",
    "                               decision_overlap = classifier.decision_overlap,\n",
    "                               X = dataset.X_valid,\n",
    "                               y = dataset.y_valid)\n",
    "        restoring_best = RestoringBest(metric=metric, monitor=monitor_metric, mode=monitor_mode)\n",
    "        callbacks.append(restoring_best)\n",
    "\n",
    "    history, y_test_prediction = classifier.train(epochs=epochs,\n",
    "                                                  X_train=dataset.X_train,\n",
    "                                                  y_train=dataset.y_train,\n",
    "                                                  X_valid=dataset.X_valid,\n",
    "                                                  y_valid=dataset.y_valid,\n",
    "                                                  X_test=dataset.X_test,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  callback=callbacks)\n",
    "\n",
    "    result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                 loss_name=classifier.get_loss_function_name(),\n",
    "                                 y_pred=y_test_prediction,\n",
    "                                 y_real_raw=dataset.y_test,\n",
    "                                 segment_size=classifier.segments_size,\n",
    "                                 segment_overlap=classifier.segments_overlap,\n",
    "                                 decision_size=classifier.decision_size,\n",
    "                                 decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    print('Classifier:%s Test(Core):%5.2f Test(MV):%5.2f ' % (str(classifier), result_test['Core']['accuracy'], result_test['MV']['accuracy']))\n",
    "\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9CZOYRH4y8U"
   },
   "source": [
    "# Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "lX06Xc3o4zQw"
   },
   "outputs": [],
   "source": [
    "def h_block_analyzer(db_path, sample_rate, features, n_classes, noise_rate,\n",
    "                     segments_time, segments_overlap,\n",
    "                     decision_time, decision_overlap,\n",
    "                     classifier, epochs, batch_size, data_length_time,\n",
    "                     monitor_metric, monitor_mode, restore_best,\n",
    "                     n_h_block, n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step=1):\n",
    "    \"\"\"\n",
    "    :param db_path: the address of dataset directory\n",
    "    :param sample_rate: the sampling rate of signals\n",
    "    :param features: the signals of original data\n",
    "    :param n_classes: the number of classes\n",
    "    :param noise_rate: the rate of noises injected to test data\n",
    "    :param segments_time: the length of each segment in seconds.\n",
    "    :param segments_overlap: the overlap of each segment\n",
    "    :param classifier: the neural network\n",
    "    :param epochs: the number of training epochs\n",
    "    :param batch_size: the number of segments in each batch\n",
    "    :param data_length_time: the length of data of each class. -1 = whole.\n",
    "    :param n_h_block: the number of all hv blocks\n",
    "    :param n_train_h_block: the number of hv blocks to train network\n",
    "    :param n_valid_h_block: the number of hv blocks to validate network\n",
    "    :param n_test_h_block: the number of hv blocks to test network\n",
    "    :param h_moving_step: the number of movement of test and validation blocks in each iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    final_statistics = {}\n",
    "    add_noise = noise_rate < 100\n",
    "\n",
    "    # Create hv blocks\n",
    "    data_blocks = [i for i in range(n_h_block)]\n",
    "    n_vt = (n_valid_h_block + n_test_h_block)\n",
    "    n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    statistics = {}\n",
    "    for i in range(n_iteration + 1):\n",
    "        print('iteration: %d/%d' % (i + 1, n_iteration + 1))\n",
    "        training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "        train_blocks = training_container[:n_train_h_block]\n",
    "        valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "        test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "        dataset = Dataset(db_path,\n",
    "                          sample_rate,\n",
    "                          features=features,\n",
    "                          window_time=segments_time,\n",
    "                          window_overlap_percentage=segments_overlap,\n",
    "                          decision_time=decision_time,\n",
    "                          decision_overlap_percentage=decision_overlap,\n",
    "                          add_noise=add_noise,\n",
    "                          noise_rate=noise_rate,\n",
    "                          train_blocks=train_blocks,\n",
    "                          valid_blocks=valid_blocks,\n",
    "                          test_blocks=test_blocks,\n",
    "                          data_length_time=data_length_time)\n",
    "        dataset.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "        logdir = os.path.join(\"./log/checkpointss/\", date_str + \"[\" + str(i) + \"]\")\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        result = train_model(dataset=dataset, classifier=classifier, epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             restore_best=restore_best,\n",
    "                             monitor_metric=monitor_metric,\n",
    "                             monitor_mode=monitor_mode,\n",
    "                             log_dir=logdir)\n",
    "\n",
    "        for key in result.keys():\n",
    "            if not key in statistics:\n",
    "                statistics[key] = {}\n",
    "            for inner_key in result[key].keys():\n",
    "                if not inner_key in statistics[key]:\n",
    "                    statistics[key][inner_key] = []\n",
    "                statistics[key][inner_key].append(result[key][inner_key])\n",
    "        shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "    print(statistics)\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TtSkS-W454c"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "t1mJTX0Y46OV"
   },
   "outputs": [],
   "source": [
    "def save_result(log_dir, data: dict):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Save to file\n",
    "    with open(log_dir + 'statistics.txt', 'a') as f:\n",
    "        f.write('\\n==========***==========\\n')\n",
    "        f.write(str(data))\n",
    "        f.write('\\n')\n",
    "\n",
    "    csv_file = log_dir + 'statistics.csv'\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    try:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(data.keys())\n",
    "            writer.writerow(data.values())\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Whr7yWQL48po"
   },
   "outputs": [],
   "source": [
    "def cumulative_average(numbers):\n",
    "    avg = []\n",
    "    for i in range(len(numbers)):\n",
    "        check = 0\n",
    "        l = i + 1\n",
    "        for j in range(i+1):\n",
    "            check += numbers[j]\n",
    "        avg.append(check/l)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo0q8aqH4-4w"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "ZzgHB2zC5Bk_"
   },
   "outputs": [],
   "source": [
    "problems = {'ConfLongDemo_JSI':{},\n",
    "            'Healthy_Older_People':{},\n",
    "            'User_Identification_From_Walking':{}\n",
    "           }\n",
    "\n",
    "problems['ConfLongDemo_JSI']['dataset'] = './datasets/ConfLongDemo_JSI/'\n",
    "problems['ConfLongDemo_JSI']['n_classes'] = 5\n",
    "problems['ConfLongDemo_JSI']['features'] = [\"x\", \"y\", \"z\"]\n",
    "problems['ConfLongDemo_JSI']['sample_rate'] = 30\n",
    "problems['ConfLongDemo_JSI']['data_length_time'] = -1\n",
    "problems['ConfLongDemo_JSI']['n_h_block'] = 10\n",
    "problems['ConfLongDemo_JSI']['n_train_h_block'] = 5\n",
    "problems['ConfLongDemo_JSI']['n_valid_h_block'] = 2\n",
    "problems['ConfLongDemo_JSI']['n_test_h_block'] = 3\n",
    "problems['ConfLongDemo_JSI']['h_moving_step'] = 1\n",
    "problems['ConfLongDemo_JSI']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60]\n",
    "problems['ConfLongDemo_JSI']['segments_times'] = [3,4,5,6,7,8,9,10,30,60]\n",
    "\n",
    "problems['Healthy_Older_People']['dataset'] = './datasets/Healthy_Older_People/'\n",
    "problems['Healthy_Older_People']['n_classes'] = 12\n",
    "problems['Healthy_Older_People']['features'] = [\"X\", \"Y\", \"Z\"]\n",
    "problems['Healthy_Older_People']['sample_rate'] = 1\n",
    "problems['Healthy_Older_People']['data_length_time'] = -1\n",
    "problems['Healthy_Older_People']['n_h_block'] = 10\n",
    "problems['Healthy_Older_People']['n_train_h_block'] = 5\n",
    "problems['Healthy_Older_People']['n_valid_h_block'] = 2\n",
    "problems['Healthy_Older_People']['n_test_h_block'] = 3\n",
    "problems['Healthy_Older_People']['h_moving_step'] = 1\n",
    "problems['Healthy_Older_People']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60]\n",
    "problems['Healthy_Older_People']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60]\n",
    "\n",
    "problems['User_Identification_From_Walking']['dataset'] = './datasets/User_Identification_From_Walking/'\n",
    "problems['User_Identification_From_Walking']['n_classes'] = 22\n",
    "problems['User_Identification_From_Walking']['features'] = [' x acceleration', ' y acceleration', ' z acceleration']\n",
    "problems['User_Identification_From_Walking']['sample_rate'] = 32\n",
    "problems['User_Identification_From_Walking']['data_length_time'] = -1\n",
    "problems['User_Identification_From_Walking']['n_h_block'] = 10\n",
    "problems['User_Identification_From_Walking']['n_train_h_block'] = 5\n",
    "problems['User_Identification_From_Walking']['n_valid_h_block'] = 2\n",
    "problems['User_Identification_From_Walking']['n_test_h_block'] = 3\n",
    "problems['User_Identification_From_Walking']['h_moving_step'] = 1\n",
    "problems['User_Identification_From_Walking']['decision_times'] = [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "problems['User_Identification_From_Walking']['segments_times'] = [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFsacgLZ5EG4"
   },
   "source": [
    "# Main Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "O8nfqyGV5GzQ"
   },
   "outputs": [],
   "source": [
    "train_config = [\n",
    "    {\n",
    "        \"loss_function\": \"categorical_crossentropy\",\n",
    "        \"loss_metric\": pesl,\n",
    "        \"monitor_metric\": \"ms_pesl\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EcygQRU5SK_"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRP2mSzx5U2I",
    "outputId": "fc2e58ee-e97f-4078-eda4-11f440fba7ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl: 0:00:04 and wl: 0:00:03\n",
      "iteration: 1/6\n",
      "./User_Identification_From_Walking/segments/method: 3d/noise: 100/n_classes:22/wl: 96/wo: 72/dl: 128/do: 0/train: [5, 6, 7, 8, 9]/valid: [0, 1]/test: [2, 3, 4]/\n",
      "Dataset is already\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 04:14:29.421762: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# cross-validation\u001b[39;00m\n\u001b[1;32m     65\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 66\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[43mh_block_analyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnoise_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msegments_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegments_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msegments_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegments_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdecision_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecision_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdecision_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecision_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdata_length_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_length_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_h_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_h_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_train_h_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_train_h_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_valid_h_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_valid_h_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_test_h_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_test_h_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mh_moving_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_moving_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmonitor_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonitor_metric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmonitor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonitor_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mrestore_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_best\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     88\u001b[0m running_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[0;32mIn[92], line 60\u001b[0m, in \u001b[0;36mh_block_analyzer\u001b[0;34m(db_path, sample_rate, features, n_classes, noise_rate, segments_time, segments_overlap, decision_time, decision_overlap, classifier, epochs, batch_size, data_length_time, monitor_metric, monitor_mode, restore_best, n_h_block, n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(logdir):\n\u001b[1;32m     59\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(logdir)\n\u001b[0;32m---> 60\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrestore_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmonitor_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmonitor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m statistics:\n",
      "Cell \u001b[0;32mIn[91], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset, classifier, epochs, batch_size, log_dir, monitor_metric, monitor_mode, restore_best)\u001b[0m\n\u001b[1;32m     15\u001b[0m     restoring_best \u001b[38;5;241m=\u001b[39m RestoringBest(metric\u001b[38;5;241m=\u001b[39mmetric, monitor\u001b[38;5;241m=\u001b[39mmonitor_metric, mode\u001b[38;5;241m=\u001b[39mmonitor_mode)\n\u001b[1;32m     16\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(restoring_best)\n\u001b[0;32m---> 18\u001b[0m history, y_test_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43my_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m result_test \u001b[38;5;241m=\u001b[39m analysis_model(loss_fn\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mget_loss_function(),\n\u001b[1;32m     28\u001b[0m                              loss_name\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mget_loss_function_name(),\n\u001b[1;32m     29\u001b[0m                              y_pred\u001b[38;5;241m=\u001b[39my_test_prediction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m                              decision_size\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mdecision_size,\n\u001b[1;32m     34\u001b[0m                              decision_overlap\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mdecision_overlap)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassifier:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Test(Core):\u001b[39m\u001b[38;5;132;01m%5.2f\u001b[39;00m\u001b[38;5;124m Test(MV):\u001b[39m\u001b[38;5;132;01m%5.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(classifier), result_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCore\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], result_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMV\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[58], line 67\u001b[0m, in \u001b[0;36mCNN_L.train\u001b[0;34m(self, epochs, X_train, y_train, X_valid, y_valid, X_test, callback, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m y_train_onehot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pd\u001b[38;5;241m.\u001b[39mget_dummies(y_train), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n\u001b[1;32m     65\u001b[0m y_valid_onehot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pd\u001b[38;5;241m.\u001b[39mget_dummies(y_valid), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m---> 67\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "noise_rate = 100\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "repetitions = 10\n",
    "restore_best = True\n",
    "tci=0\n",
    "\n",
    "for problem in ['User_Identification_From_Walking','Healthy_Older_People', 'ConfLongDemo_JSI']:\n",
    "    log_dir = f\"./benchmark/logs/baselines/fusion/{problem}/\"\n",
    "    dataset = problems[problem]['dataset']\n",
    "    n_classes = problems[problem]['n_classes']\n",
    "    features = problems[problem]['features']\n",
    "    sample_rate = problems[problem]['sample_rate']\n",
    "    data_length_time = problems[problem]['data_length_time']\n",
    "    n_h_block = problems[problem]['n_h_block']\n",
    "    n_train_h_block = problems[problem]['n_train_h_block']\n",
    "    n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "    n_test_h_block = problems[problem]['n_test_h_block']\n",
    "    h_moving_step = problems[problem]['h_moving_step']\n",
    "    decision_times = problems[problem]['decision_times']\n",
    "    segments_times = problems[problem]['segments_times']\n",
    "    \n",
    "    for model in models:\n",
    "        for decision_time in decision_times:\n",
    "            for decision_overlap in [0.0]:\n",
    "                for segments_time in segments_times:\n",
    "                    for segments_overlap in [0.75]:\n",
    "                        if float(decision_time*0.75) < float(segments_time):\n",
    "                            continue\n",
    "                        \n",
    "                        if (os.path.exists(log_dir + \"statistics.csv\")):\n",
    "                            is_investigated = False\n",
    "                            saved_statistics = pd.read_csv(log_dir + \"statistics.csv\")\n",
    "                            for index, row in saved_statistics.iterrows():\n",
    "                                if row['dataset'] == str(dataset) and row['inner_classifier'] == str(model):\n",
    "                                    t1 = datetime.strptime(row['segments_time'],\"%H:%M:%S\")\n",
    "                                    t2 = datetime.strptime(row['decision_time'],\"%H:%M:%S\")\n",
    "                                    td1 = timedelta(hours=t1.hour, minutes=t1.minute, seconds=t1.second)\n",
    "                                    td2 = timedelta(hours=t2.hour, minutes=t2.minute, seconds=t2.second)\n",
    "                                    if (td1.total_seconds() == int(segments_time) \n",
    "                                        and td2.total_seconds() == int(decision_time)):\n",
    "                                        print(\"dataset: \" + problem + \" \" +\n",
    "                                              \"model: \" + model + \" \" +\n",
    "                                            \"dl: \" + row['decision_time'] + \" and wl:\" + row[\n",
    "                                            'segments_time'] + \" is investigated!\")\n",
    "                                        is_investigated = True\n",
    "                                        break\n",
    "                            if (is_investigated):\n",
    "                                continue\n",
    "\n",
    "                        print(\"dl: \" + str(timedelta(seconds=int(decision_time))) +\n",
    "                              \" and \" +\n",
    "                              \"wl: \" + str(timedelta(seconds=int(segments_time))))\n",
    "                        \n",
    "                        classifier = eval(model)(classes=n_classes,\n",
    "                                                 n_features=len(features),\n",
    "                                                 segments_size=int(segments_time * sample_rate),\n",
    "                                                 segments_overlap=segments_overlap,\n",
    "                                                 decision_size=int(decision_time * sample_rate),\n",
    "                                                 decision_overlap=decision_overlap,\n",
    "                                                 loss_metric=train_config[0][\"loss_metric\"],\n",
    "                                                 loss_function=train_config[0][\"loss_function\"])\n",
    "                        # cross-validation\n",
    "                        start = datetime.now()\n",
    "                        statistics = h_block_analyzer(db_path=dataset,\n",
    "                                                      sample_rate=sample_rate,\n",
    "                                                      features=features,\n",
    "                                                      n_classes=n_classes,\n",
    "                                                      noise_rate=noise_rate,\n",
    "                                                      segments_time=segments_time,\n",
    "                                                      segments_overlap=segments_overlap,\n",
    "                                                      decision_time=decision_time,\n",
    "                                                      decision_overlap=decision_overlap,\n",
    "                                                      classifier=classifier,\n",
    "                                                      epochs=epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      data_length_time=data_length_time,\n",
    "                                                      n_h_block=n_h_block,\n",
    "                                                      n_train_h_block=n_train_h_block,\n",
    "                                                      n_valid_h_block=n_valid_h_block,\n",
    "                                                      n_test_h_block=n_test_h_block,\n",
    "                                                      h_moving_step=h_moving_step,\n",
    "                                                      monitor_metric=train_config[0][\"monitor_metric\"],\n",
    "                                                      monitor_mode=train_config[0][\"monitor_mode\"],\n",
    "                                                      restore_best=restore_best)\n",
    "                        end = datetime.now()\n",
    "                        running_time = end - start\n",
    "                        # Summarizing the results of cross-validation\n",
    "                        data = {}\n",
    "                        data['dataset'] = dataset\n",
    "                        data['class'] = str(n_classes)\n",
    "                        loss_metric = train_config[0][\"loss_metric\"]\n",
    "                        data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "                        loss_function = train_config[0][\"loss_function\"]\n",
    "                        data['loss_function'] = loss_function if type(loss_function) == str else loss_function.__name__\n",
    "                        data['monitor_metric'] = train_config[0][\"monitor_metric\"]\n",
    "                        data['monitor_mode'] = train_config[0][\"monitor_mode\"]\n",
    "                        data['restore_best'] = str(restore_best)\n",
    "                        data['features'] = str(features)\n",
    "                        data['sample_rate'] = str(sample_rate)\n",
    "                        data['noise_rate'] = str(noise_rate)\n",
    "                        data['epochs'] = str(epochs)\n",
    "                        data['batch_size'] = str(batch_size)\n",
    "                        data['data_length_time'] = str(data_length_time)\n",
    "                        data['repetitions'] = str(repetitions)\n",
    "                        data['n_h_block'] = str(n_h_block)\n",
    "                        data['n_train_h_block'] = str(n_train_h_block)\n",
    "                        data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                        data['n_test_h_block'] = str(n_test_h_block)\n",
    "                        data['h_moving_step'] = str(h_moving_step)\n",
    "                        data['segments_time'] = str(segments_time)\n",
    "                        data['segments_overlap'] = str(segments_overlap)\n",
    "                        data['inner_classifier'] = str(model)\n",
    "                        data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                        data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                        data['n_params'] = classifier.count_params()\n",
    "                        data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                        data['segments_overlap'] = segments_overlap\n",
    "                        data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                        data['decision_overlap'] = decision_overlap\n",
    "                        statistics_summary = {}\n",
    "                        for key in statistics.keys():\n",
    "                            for inner_key in statistics[key].keys():\n",
    "                                statistics_summary[key + '_' + inner_key + '_mean'] = np.average(statistics[key][inner_key])\n",
    "                                statistics_summary[key + '_' + inner_key + '_std'] = np.std(statistics[key][inner_key])\n",
    "                                statistics_summary[key + '_' + inner_key + '_max'] = np.max(statistics[key][inner_key])\n",
    "                                statistics_summary[key + '_' + inner_key + '_min'] = np.min(statistics[key][inner_key])\n",
    "                        data.update(statistics_summary)\n",
    "                        # Save information\n",
    "                        save_result(log_dir=log_dir, data=data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qnEf299z3qfE",
    "J3yxSrGp3u-E",
    "rsqjDnvY30UY",
    "fTAGp_3s364X",
    "9KP0pEQM4KNT",
    "0TtSkS-W454c",
    "oo0q8aqH4-4w",
    "BHeTm0ql5Oye"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
